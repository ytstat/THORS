{
    "collab_server" : "",
    "contents" : "#' Construct a cost-sensitive classifier from training samples and validation samples.\n#'\n#' Given the training data, the validation data, the cost matrix and the form of the base classifier,\n#' \\code{thors} can build a cost-sensitive classifierbased on the thresholding strategy.\n#' @export\n#' @importFrom e1071 svm\n#' @importFrom e1071 naiveBayes\n#' @importFrom e1071 permutations\n#' @importFrom class knn\n#' @importFrom naivebayes naive_bayes\n#' @importFrom glmnet cv.glmnet\n#' @importFrom MASS lda\n#' @importFrom randomForest randomForest\n#' @importFrom ada ada\n#' @importFrom parallel detectCores\n#' @importFrom parallel makeCluster\n#' @importFrom parallel stopCluster\n#' @importFrom doParallel registerDoParallel\n#' @import foreach\n#' @import doParallel\n#' @importFrom graphics plot\n#' @importFrom stats glm\n#' @importFrom stats predict\n#' @importFrom stats pbinom\n#' @importFrom stats rnorm\n#' @importFrom stats sd\n#' @importFrom stats rbinom\n#' @importFrom tree tree\n#' @importFrom graphics polygon\n#' @importFrom nnet multinom\n#' @importFrom caret createFolds\n#' @param xtrain the n * p observation matrix for training the base classifier. n observations, p features.\n#' @param ytrain n 2-class labels for training the base classifier.\n#' @param xvalid the n * p observation matrix for finding the optimal threshold. IF \\code{NULL}, cv must be assign an positive integer.\n#' @param yvalid n 2-class labels for finding the optimal threshold. IF \\code{NULL}, cv must be assign an positive integer.\n#' @param beta the cost ratio of FP and FN cases. Default = 0.5.\n#' @param cv the indicator for choosing the threshold based on the cross-validation. If \\code{FALSE}, \\code{xvalid} and \\code{yvalid} can not be NUll. If cv is assigned a specific integer \\code{a}, then THORS will be applied based on a-fold cross validation and the threshold is the average value of thresholds chosen in each fold. Default = \\code{FALSE}.\n#' @param mrg.est the estimation method for estimating the marginal probabilities of two classes. IF \\code{train}, the marginal probabilities are estimated based on the training set. If \\code{valid}, the marginal probabilities are estimated based on the validation set. Default = \\code{\"train\"}.\n#' @param cores how many cores to be used. If \\code{\"auto\"}, all the cores detected will be used. Default = \\code{\"auto\"}.\n#' @param base the base classifier chosen. Default = \\code{\"logistic\"}.\n#' \\itemize{\n#' \\item logistic: Multi-logistic regression. \\link{multinom} in \\code{nnet} package.\n#' \\item svm: Support Vector Machines. \\code{\\link[e1071]{svm}} in \\code{e1071} package.\n#' \\item knn: k-Nearest Neighbor classifier. \\code{\\link[class]{knn}} in \\code{knn} package.\n#' \\item randomforest: Random Forest. \\code{\\link[randomForest]{randomForest}} in \\code{randomForest} package.\n#' \\item lda: Linear Discriminant Analysis. \\code{\\link[MASS]{lda}} in \\code{MASS} package.\n#' \\item nb: Naive Bayes. \\code{\\link[e1071]{naiveBayes}} in \\code{e1071} package.\n#' \\item tree: Decision Tree. \\code{\\link[tree]{tree}} in \\code{tree} package.\n#' }\n#' @param ... additional arguments.\n#' @return An object with S3 class thors.\n#'  \\item{fit}{a list including the fitting paramters, the threshold, the base classifier type, the method for estimating mariginal probabilities, and the indicator of cross-validation.}\n#'  \\item{threshold}{the threshold chosen for the binary classifcation}\n#'  \\item{base}{the base classifier type chosen.}\n#'  \\item{mrg.est}{the method for estimating the marginal probabilities.}\n#'  \\item{cv}{the indicator of cross-validation.}\n#' @references Tian, Y., & Zhang, W. (2018). THORS: An Efficient Approach for Making Classifiers Cost-sensitive. arXiv preprint arXiv:1811.028\n#' @seealso \\code{\\link{predict.thors}}, \\code{\\link{cost}}.\n#' @examples\n#' ## calculate the threshold based on the validation data directly\n#' library(datasets)\n#' data(iris)\n#' set.seed(1)\n#' beta <- 0.5\n#' D <- iris[1:100,][sample(100),]\n#' D$Species <- as.numeric(D$Species)-1 # assign class \"setosa\" as class 0 and class \"versicolor\" as class 1.\n#' xtrain <- D[1:40,]\n#' ytrain <- D$Species[1:40]\n#' xvalid <- D[40+1:40,]\n#' yvalid <- D$Species[40+1:40]\n#' xtest <- D[80+1:20,]\n#' ytest <- D$Species[80+1:20]\n#' fit <- thors(xtrain, ytrain, xvalid, yvalid,  beta = beta, cv = FALSE, base = \"logistic\")\n#' ypred <- predict(fit, xtest)\n#' cost(ytest, ypred, beta = beta, A=10, form = \"average\")\n#'\n#' \\dontrun{\n#' ## use the 5-fold cross-validation to calculate the threshold\n#' library(datasets)\n#' data(iris)\n#' set.seed(1)\n#' beta <- 0.5\n#' D <- iris[1:100,][sample(100),]\n#' D$Species <- as.numeric(D$Species)-1 # assign class \"setosa\" as class 0 and class \"versicolor\" as class 1.\n#' xtrain <- D[1:80,]\n#' ytrain <- D$Species[1:80]\n#' xtest <- D[80+1:20,]\n#' ytest <- D$Species[80+1:20]\n#' fit <- thors(xtrain, ytrain, xvalid = NULL, yvalid = NULL,  beta = beta, cv = 5, base = \"logistic\")\n#' ypred <- predict(fit, xtest)\n#' cost(ytest, ypred, beta = beta, A=10, form = \"average\")\n#' }\n\nthors <- function(xtrain, ytrain, xvalid, yvalid,  beta = 0.5,\n                  cv = FALSE, mrg.est = c(\"train\", \"valid\"), cores = \"auto\",\n                  base = c(\"logistic\", \"svm\", \"knn\", \"randomforest\", \"lda\", \"nb\", \"tree\"), ...){\n  base <- match.arg(base)\n  mrg.est <- match.arg(mrg.est)\n  if(cores == \"auto\"){\n    num.cores <- detectCores()\n  }else{\n    num.cores <- cores\n  }\n  cl <- makeCluster(num.cores)\n  registerDoParallel(cl)\n  if(cv){\n    folds <- createFolds(1:length(ytrain), k = cv)\n    Dtrain <- data.frame(xtrain, class=as.factor(ytrain))\n    t.list <- foreach(k = 1:cv, .combine = \"c\",\n                      .packages = c(\"MASS\", \"tree\", \"naivebayes\", \"caret\", \"e1071\",\n                                    \"randomForest\", \"nnet\")) %dopar% {\n      if(base==\"logistic\"){\n        fit <- suppressWarnings(glm(class~., data=Dtrain[-folds[[k]],], family = \"binomial\"))\n        xvalid <- xtrain[folds[[k]],]\n        yvalid <- ytrain[folds[[k]]]\n        score.valid <- predict(fit,xvalid,type=\"response\")\n      }else if(base==\"lda\"){\n        fit <- lda(class~., data=Dtrain[-folds[[k]],])\n        xvalid <- xtrain[folds[[k]],]\n        yvalid <- ytrain[folds[[k]]]\n        score.valid <- predict(fit, xvalid)$posterior[,2]\n      }else if(base==\"svm\"){\n        fit <- svm(class~., data=Dtrain[-folds[[k]],], probability=TRUE, kernel = \"linear\")\n        xvalid <- xtrain[folds[[k]],]\n        yvalid <- ytrain[folds[[k]]]\n        score.valid <- attr(predict(fit, xvalid, probability = TRUE), \"probabilities\")[,2]\n      }else if(base == \"randomforest\"){\n        fit <- randomForest(class~., data=Dtrain[-folds[[k]],], ...)\n        xvalid <- xtrain[folds[[k]],]\n        yvalid <- ytrain[folds[[k]]]\n        score.valid <- predict(fit, xvalid, type=\"prob\")[,2]\n      }else if(base == \"nb\"){\n        fit <- naive_bayes(class~., data=Dtrain[-folds[[k]],], usekernel = FALSE)\n        xvalid <- xtrain[folds[[k]],]\n        yvalid <- ytrain[folds[[k]]]\n        score.valid <- predict(fit, xvalid, type=\"prob\")[,2]\n      }else if(base == \"knn\"){\n        fit <- knn3(class~., data=Dtrain[-folds[[k]],], k = 10)\n        xvalid <- xtrain[folds[[k]],]\n        yvalid <- ytrain[folds[[k]]]\n        score.valid <- predict(fit, xvalid, type=\"prob\")[,2]\n      }else if(base == \"tree\"){\n        fit <- tree(class~., data=Dtrain[-folds[[k]],])\n        xvalid <- xtrain[folds[[k]],]\n        yvalid <- ytrain[folds[[k]]]\n        score.valid <- predict(fit, xvalid, type=\"vector\")[,2]\n      }\n\n      if(mrg.est==\"train\"){\n        xtrain.cv <- xtrain[-folds[[k]],]\n        ytrain.cv <- ytrain[-folds[[k]]]\n        p0 <- sum(ytrain.cv == 0)/length(ytrain.cv)\n        p1 <- 1-p0\n      }else{\n        p0 <- sum(yvalid == 0)/length(yvalid)\n        p1 <- 1-p0\n      }\n      n0 <- sum(yvalid == 0)\n      n1 <- sum(yvalid == 1)\n\n      s <- score.valid[order(score.valid)]\n      s0 <- score.valid[yvalid==0]\n      s1 <- score.valid[yvalid==1]\n      s0 <- s0[order(s0)]\n      s1 <- s1[order(s1)]\n\n\n      C <- sapply(1:length(s), function(i){\n        p1*length(s1[s1<=s[i]])/n1 - beta*p0*length(s0[s0<=s[i]])/n0\n      })\n      s[which.min(C)]\n    }\n\n    if(base==\"logistic\"){\n      fit <- suppressWarnings(glm(class~., data=Dtrain, family = \"binomial\"))\n    }else if(base==\"lda\"){\n      fit <- lda(class~., data=Dtrain)\n    }else if(base==\"svm\"){\n      fit <- svm(class~.,data=Dtrain,probability=TRUE)\n    }else if(base == \"randomforest\"){\n      fit <- randomForest(class~., data=Dtrain, ...)\n    }else if(base == \"nb\"){\n      fit <- naive_bayes(class~., data=Dtrain, usekernel = FALSE)\n    }else if(base == \"knn\"){\n      fit <- knn3(class~., data=Dtrain, k = 10)\n    }else if(base == \"tree\"){\n      fit <- tree(class~.,data=Dtrain)\n    }\n    t <- mean(t.list)\n  }else{\n    if(mrg.est==\"train\"){\n      p0 <- sum(ytrain == 0)/length(ytrain)\n      p1 <- 1-p0\n    }else{\n      p0 <- sum(yvalid == 0)/length(yvalid)\n      p1 <- 1-p0\n    }\n    n0 <- sum(yvalid == 0)\n    n1 <- sum(yvalid == 1)\n    Dtrain <- data.frame(xtrain,class=factor(ytrain))\n    if(base == \"logistic\") {\n      fit <- suppressWarnings(glm(class~., data=Dtrain, family = \"binomial\"))\n      score.valid <- predict(fit,xvalid,type=\"response\")\n    }else if(base == \"lda\"){\n      fit <- lda(class~., data=Dtrain)\n      score.valid <- predict(fit, xvalid)$posterior[,2]\n    }else if(base==\"svm\"){\n      fit <- svm(class~.,data=Dtrain, probability=TRUE, kernel = \"linear\")\n      score.valid <- attr(predict(fit, xvalid, probability = TRUE), \"probabilities\")[,2]\n    }else if(base == \"randomforest\"){\n      fit <- randomForest(class~., data=Dtrain)\n      score.valid <- predict(fit, xvalid, type=\"prob\")[,2]\n    }else if(base == \"nb\"){\n      fit <- naive_bayes(class~., data=Dtrain, usekernel = FALSE)\n      score.valid <- predict(fit, xvalid, type=\"prob\")[,2]\n    }else if(base == \"knn\"){\n      fit <- knn3(class~., data=Dtrain, k = 10)\n      score.valid <- predict(fit, xvalid, type=\"prob\")[,2]\n    }else if(base == \"tree\"){\n      fit <- tree(class~.,data=Dtrain)\n      score.valid <- predict(fit, xvalid, type=\"vector\")[,2]\n    }\n\n\n    s <- score.valid[order(score.valid)]\n    s0 <- score.valid[yvalid==0]\n    s1 <- score.valid[yvalid==1]\n    s0 <- s0[order(s0)]\n    s1 <- s1[order(s1)]\n\n    C <- foreach(i = 1:length(s), .combine = \"c\") %dopar% {\n      p1*length(s1[s1<=s[i]])/n1 - beta*p0*length(s0[s0<=s[i]])/n0\n    }\n\n    t <- s[which.min(C)]\n  }\n\n  object <- list(fits=fit, threshold=t, base=base, mrg.est=mrg.est, cv=cv)\n  class(object) <- \"thors\"\n\n  stopCluster(cl)\n\n  object\n}\n\n\n",
    "created" : 1560239265742.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "17936431",
    "id" : "A071756D",
    "lastKnownWriteTime" : 1560239383,
    "last_content_update" : 1560239383713,
    "path" : "C:/Users/tiany/Desktop/THORS/R/thors.R",
    "project_path" : "R/thors.R",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}